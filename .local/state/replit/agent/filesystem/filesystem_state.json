{"file_contents":{"app.py":{"content":"import os\nimport logging\nfrom flask import Flask, render_template, jsonify, request, redirect, url_for, flash\nfrom werkzeug.middleware.proxy_fix import ProxyFix\nfrom scraper import CompetitorScraper\nimport json\nfrom datetime import datetime, timezone\nimport dateutil.parser\n\n# Configure logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Create the app\napp = Flask(__name__)\napp.secret_key = os.environ.get(\"SESSION_SECRET\", \"dev-secret-key-12345\")\napp.wsgi_app = ProxyFix(app.wsgi_app, x_proto=1, x_host=1)\n\n# Initialize the scraper\nscraper = CompetitorScraper()\n\n# Custom template filters\n@app.template_filter('from_iso')\ndef from_iso_filter(date_string):\n    \"\"\"Convert ISO date string to datetime object\"\"\"\n    if not date_string:\n        return datetime.now()\n    try:\n        return dateutil.parser.parse(date_string)\n    except:\n        return datetime.now()\n\n@app.template_filter('timesince')\ndef timesince_filter(dt):\n    \"\"\"Calculate time since given datetime\"\"\"\n    if not dt:\n        return \"unknown\"\n    \n    if isinstance(dt, str):\n        try:\n            dt = dateutil.parser.parse(dt)\n        except:\n            return \"unknown\"\n    \n    now = datetime.now(timezone.utc)\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=timezone.utc)\n    \n    diff = now - dt\n    \n    if diff.days > 0:\n        return f\"{diff.days} day{'s' if diff.days != 1 else ''}\"\n    elif diff.seconds > 3600:\n        hours = diff.seconds // 3600\n        return f\"{hours} hour{'s' if hours != 1 else ''}\"\n    elif diff.seconds > 60:\n        minutes = diff.seconds // 60\n        return f\"{minutes} minute{'s' if minutes != 1 else ''}\"\n    else:\n        return \"just now\"\n\n@app.route('/')\ndef index():\n    \"\"\"Main dashboard showing competitor pricing data\"\"\"\n    try:\n        data = scraper.get_all_data()\n        return render_template('index.html', data=data)\n    except Exception as e:\n        logging.error(f\"Error loading dashboard: {str(e)}\")\n        flash(f\"Error loading data: {str(e)}\", \"error\")\n        return render_template('index.html', data={})\n\n@app.route('/refresh', methods=['POST'])\ndef refresh_data():\n    \"\"\"Manually trigger a refresh of all competitor data\"\"\"\n    try:\n        logging.info(\"Starting manual refresh of competitor data\")\n        results = scraper.scrape_all()\n        \n        success_count = sum(1 for result in results.values() if result.get('success'))\n        total_count = len(results)\n        \n        if success_count == total_count:\n            flash(f\"Successfully updated all {success_count} competitors\", \"success\")\n        elif success_count > 0:\n            flash(f\"Updated {success_count} out of {total_count} competitors. Check logs for errors.\", \"warning\")\n        else:\n            flash(\"Failed to update any competitor data. Check logs for errors.\", \"error\")\n            \n        logging.info(f\"Refresh completed: {success_count}/{total_count} successful\")\n        \n    except Exception as e:\n        logging.error(f\"Error during refresh: {str(e)}\")\n        flash(f\"Error during refresh: {str(e)}\", \"error\")\n    \n    return redirect(url_for('index'))\n\n@app.route('/refresh/<competitor>')\ndef refresh_single(competitor):\n    \"\"\"Refresh data for a single competitor\"\"\"\n    try:\n        result = scraper.scrape_single(competitor)\n        if result.get('success'):\n            flash(f\"Successfully updated {competitor}\", \"success\")\n        else:\n            flash(f\"Failed to update {competitor}: {result.get('error', 'Unknown error')}\", \"error\")\n    except Exception as e:\n        logging.error(f\"Error refreshing {competitor}: {str(e)}\")\n        flash(f\"Error refreshing {competitor}: {str(e)}\", \"error\")\n    \n    return redirect(url_for('index'))\n\n@app.route('/export')\ndef export_data():\n    \"\"\"Export all competitor data as JSON\"\"\"\n    try:\n        data = scraper.get_all_data()\n        return jsonify(data), 200, {\n            'Content-Type': 'application/json',\n            'Content-Disposition': f'attachment; filename=competitor_pricing_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n        }\n    except Exception as e:\n        logging.error(f\"Error exporting data: {str(e)}\")\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/api/data')\ndef api_data():\n    \"\"\"API endpoint to get all data\"\"\"\n    try:\n        data = scraper.get_all_data()\n        return jsonify(data)\n    except Exception as e:\n        logging.error(f\"Error getting API data: {str(e)}\")\n        return jsonify({'error': str(e)}), 500\n\n@app.errorhandler(404)\ndef not_found_error(error):\n    return render_template('index.html', data={}), 404\n\n@app.errorhandler(500)\ndef internal_error(error):\n    logging.error(f\"Internal server error: {str(error)}\")\n    flash(\"An internal server error occurred\", \"error\")\n    return render_template('index.html', data={}), 500\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000, debug=True)\n","size_bytes":4878},"main.py":{"content":"from app import app\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000, debug=True)\n","size_bytes":99},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"beautifulsoup4>=4.13.4\",\n    \"email-validator>=2.2.0\",\n    \"flask>=3.1.1\",\n    \"flask-sqlalchemy>=3.1.1\",\n    \"gunicorn>=23.0.0\",\n    \"psycopg2-binary>=2.9.10\",\n    \"python-dateutil>=2.9.0.post0\",\n    \"requests>=2.32.4\",\n    \"trafilatura>=2.0.0\",\n    \"werkzeug>=3.1.3\",\n]\n","size_bytes":419},"replit.md":{"content":"# Competitor Pricing Monitor\n\n## Overview\n\nThis is a Flask-based web scraper application that monitors competitor pricing data across multiple equity management and legal tech companies. The application provides a dashboard interface to view scraped pricing information and allows manual refresh of competitor data. It's designed to help track pricing strategies and market positioning by automatically extracting pricing information from competitor websites.\n\n## User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n## System Architecture\n\n**Web Framework**: Built with Flask as a lightweight Python web framework, suitable for the simple dashboard requirements and scraping operations.\n\n**Frontend Architecture**: Uses server-side rendering with Jinja2 templates and Bootstrap for styling. The frontend consists of:\n- Base template with dark theme Bootstrap CSS\n- Dashboard interface showing competitor pricing cards\n- Manual refresh functionality via POST requests\n- Flash messaging for user feedback\n\n**Scraping Engine**: Custom `CompetitorScraper` class that handles web scraping operations:\n- Requests library for HTTP operations with browser-like headers\n- BeautifulSoup for HTML parsing\n- Trafilatura for content extraction\n- In-memory data storage (no persistent database)\n- Configurable competitor list with URLs and display names\n\n**Data Management**: Uses in-memory storage for scraped data, meaning data is lost on application restart. This suggests the application is designed for real-time monitoring rather than historical analysis.\n\n**Error Handling**: Implements comprehensive error handling with logging and user feedback through Flask's flash messaging system.\n\n## External Dependencies\n\n**Core Dependencies**:\n- Flask - Web framework\n- Requests - HTTP client for web scraping\n- BeautifulSoup4 - HTML parsing\n- Trafilatura - Content extraction\n\n**Frontend Dependencies**:\n- Bootstrap CSS (via CDN) - UI framework with dark theme\n- Bootstrap Icons - Icon library\n- Chart.js - Charting library (included but not actively used in visible code)\n\n**Target Websites**: Currently configured to scrape pricing data from:\n- Carta (carta.com)\n- Bolago (bolago.com) \n- NVR (nvr.se)\n- Ledgy (ledgy.com)\n- Cake Equity (cakeequity.com)\n- Mantle (withmantle.com)\n- Clara (clara.co)\n\n**Infrastructure**: Designed to run on Replit with ProxyFix middleware for proper header handling in the hosted environment.","size_bytes":2436},"scraper.py":{"content":"import requests\nfrom bs4 import BeautifulSoup\nimport time\nimport logging\nfrom datetime import datetime\nimport re\nfrom urllib.parse import urljoin, urlparse\nimport trafilatura\n\nclass CompetitorScraper:\n    def __init__(self):\n        self.competitors = {\n            'carta': {\n                'url': 'https://carta.com/uk/en/plans/pricing-for-companies/',\n                'name': 'Carta'\n            },\n            'bolago': {\n                'url': 'https://bolago.com/se/priser/',\n                'name': 'Bolago'\n            },\n            'nvr': {\n                'url': 'https://www.nvr.se/pris',\n                'name': 'NVR'\n            },\n            'ledgy': {\n                'url': 'https://ledgy.com/company-pricing',\n                'name': 'Ledgy'\n            },\n            'cakeequity': {\n                'url': 'https://www.cakeequity.com/pricing',\n                'name': 'Cake Equity'\n            },\n            'mantle': {\n                'url': 'https://withmantle.com/pricing',\n                'name': 'Mantle'\n            },\n\n        }\n        \n        # In-memory storage for scraped data\n        self.data = {}\n        \n        # Request headers to appear more like a real browser\n        self.headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n            'Accept-Language': 'en-US,en;q=0.5',\n            'Accept-Encoding': 'gzip, deflate',\n            'Connection': 'keep-alive',\n        }\n        \n        # Rate limiting - wait between requests\n        self.request_delay = 2  # seconds\n        \n        logging.info(\"CompetitorScraper initialized\")\n\n    def scrape_single(self, competitor_key):\n        \"\"\"Scrape data for a single competitor\"\"\"\n        if competitor_key not in self.competitors:\n            return {'success': False, 'error': f'Unknown competitor: {competitor_key}'}\n        \n        competitor = self.competitors[competitor_key]\n        url = competitor['url']\n        name = competitor['name']\n        \n        logging.info(f\"Scraping {name} at {url}\")\n        \n        try:\n            # Rate limiting\n            time.sleep(self.request_delay)\n            \n            # Make request\n            response = requests.get(url, headers=self.headers, timeout=30)\n            response.raise_for_status()\n            \n            # Parse HTML\n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # Extract pricing information based on competitor\n            pricing_data = self._extract_pricing_data(competitor_key, soup, response.text)\n            \n            # Store the data\n            self.data[competitor_key] = {\n                'name': name,\n                'url': url,\n                'last_updated': datetime.now().isoformat(),\n                'success': True,\n                'pricing_data': pricing_data,\n                'error': None\n            }\n            \n            logging.info(f\"Successfully scraped {name}\")\n            return {'success': True, 'data': pricing_data}\n            \n        except requests.RequestException as e:\n            error_msg = f\"Request failed for {name}: {str(e)}\"\n            logging.error(error_msg)\n            \n            self.data[competitor_key] = {\n                'name': name,\n                'url': url,\n                'last_updated': datetime.now().isoformat(),\n                'success': False,\n                'pricing_data': None,\n                'error': error_msg\n            }\n            \n            return {'success': False, 'error': error_msg}\n            \n        except Exception as e:\n            error_msg = f\"Parsing failed for {name}: {str(e)}\"\n            logging.error(error_msg)\n            \n            self.data[competitor_key] = {\n                'name': name,\n                'url': url,\n                'last_updated': datetime.now().isoformat(),\n                'success': False,\n                'pricing_data': None,\n                'error': error_msg\n            }\n            \n            return {'success': False, 'error': error_msg}\n\n    def _extract_pricing_data(self, competitor_key, soup, raw_html):\n        \"\"\"Extract pricing information based on the competitor\"\"\"\n        \n        if competitor_key == 'carta':\n            return self._extract_carta_pricing(soup)\n        elif competitor_key == 'bolago':\n            return self._extract_bolago_pricing(soup)\n        elif competitor_key == 'nvr':\n            return self._extract_nvr_pricing(soup)\n        elif competitor_key == 'ledgy':\n            return self._extract_ledgy_pricing(soup)\n        elif competitor_key == 'cakeequity':\n            return self._extract_cakeequity_pricing(soup)\n        elif competitor_key == 'mantle':\n            return self._extract_mantle_pricing(soup)\n\n        else:\n            # Fallback: extract general pricing information\n            return self._extract_generic_pricing(soup, raw_html)\n\n    def _extract_carta_pricing(self, soup):\n        \"\"\"Extract Carta pricing information\"\"\"\n        plans = []\n        \n        # Look for pricing plan sections\n        plan_elements = soup.find_all(['div', 'section'], class_=re.compile(r'plan|pricing|card', re.I))\n        \n        # Also try to find text content that looks like pricing\n        text_content = soup.get_text()\n        \n        # Extract plan information from the actual content we know exists\n        if \"Raise\" in text_content:\n            plans.append({\n                'name': 'Raise',\n                'price': '£21/month',\n                'description': 'Up to five stakeholders (£250/year)',\n                'features': ['Cap table management', 'Advance Assurance', 'Round modelling']\n            })\n        \n        if \"Build\" in text_content:\n            plans.append({\n                'name': 'Build',\n                'price': 'Contact for pricing',\n                'description': 'Ideal for early-stage startups',\n                'features': ['Everything in Raise', 'Round closing', 'S/EIS']\n            })\n        \n        if \"Grow\" in text_content:\n            plans.append({\n                'name': 'Grow',\n                'price': 'Contact for pricing',\n                'description': 'Essentials for growing companies',\n                'features': ['Everything in Build', 'EMI & CSOP valuations', 'EMI share plans']\n            })\n        \n        if \"Scale\" in text_content:\n            plans.append({\n                'name': 'Scale',\n                'price': 'Contact for pricing',\n                'description': 'Features for scaling businesses',\n                'features': ['Everything in Grow', '409A & growth share valuations', 'Compensation management']\n            })\n        \n        return {\n            'plans': plans,\n            'currency': 'GBP',\n            'billing_period': 'monthly',\n            'raw_text_extract': self._get_pricing_text_extract(soup)\n        }\n\n    def _extract_bolago_pricing(self, soup):\n        \"\"\"Extract Bolago pricing information\"\"\"\n        plans = []\n        text_content = soup.get_text()\n        \n        # Extract known pricing from the content\n        if \"Gratis\" in text_content:\n            plans.append({\n                'name': 'Gratis',\n                'price': '0 kr',\n                'description': 'Alltid gratis',\n                'features': ['Aktiebok (till 5 aktieägare)', 'Hämtar ärenden från Bolagsverket', 'Tillgång till avtalsmallar']\n            })\n        \n        if \"Starter\" in text_content and \"395 kr\" in text_content:\n            plans.append({\n                'name': 'Starter',\n                'price': '329 kr/month',\n                'description': '12 månaders bindningstid (3,950 kr/year)',\n                'features': ['Aktiebok (till 15 aktieägare)', 'Upp till 2 användare', 'Dokumenthantering', 'E-Signatur']\n            })\n        \n        if \"Grow\" in text_content and \"1 695 kr\" in text_content:\n            plans.append({\n                'name': 'Grow',\n                'price': '1,413 kr/month',\n                'description': '12 månaders bindningstid (16,950 kr/year)',\n                'features': ['Aktiebok (till 25 aktieägare)', 'Styrelseportal', 'Bolagsstämmor', 'Optionsprogram']\n            })\n        \n        if \"Pro\" in text_content:\n            plans.append({\n                'name': 'Pro',\n                'price': 'Offert',\n                'description': 'Alltid 12 månader i taget',\n                'features': ['Obegränsat med användare', 'Skräddarsydda upplägg', 'Juridiskt konsultstöd']\n            })\n        \n        return {\n            'plans': plans,\n            'currency': 'SEK',\n            'billing_period': 'annual',\n            'raw_text_extract': self._get_pricing_text_extract(soup)\n        }\n\n    def _extract_nvr_pricing(self, soup):\n        \"\"\"Extract NVR pricing information\"\"\"\n        plans = []\n        text_content = soup.get_text()\n        \n        if \"Basic\" in text_content and \"0 kr/månad\" in text_content:\n            plans.append({\n                'name': 'Basic',\n                'price': '0 kr/månad',\n                'description': 'För bolag med få aktieägare och förändringar',\n                'features': ['Digital aktiebok', 'Aktiebok som PDF', 'Investor relations', 'Synk med Skatteverket']\n            })\n        \n        if \"Starter\" in text_content and \"49 kr/månad\" in text_content:\n            plans.append({\n                'name': 'Starter',\n                'price': 'Från 49 kr/månad',\n                'description': 'Betala per aktieägare',\n                'features': ['Kategorisering av aktieägare', 'Översiktssida med statistik', 'Rapporter i PDF och Excel']\n            })\n        \n        if \"Pro\" in text_content and \"750 kr/månad\" in text_content:\n            plans.append({\n                'name': 'Pro',\n                'price': 'Från 750 kr/månad',\n                'description': 'Betala per stakeholder',\n                'features': ['Optioner och derivat', 'Avancerade överlåtelser', 'Prioriterad support']\n            })\n        \n        return {\n            'plans': plans,\n            'currency': 'SEK',\n            'billing_period': 'monthly',\n            'raw_text_extract': self._get_pricing_text_extract(soup)\n        }\n\n    def _extract_ledgy_pricing(self, soup):\n        \"\"\"Extract Ledgy pricing information\"\"\"\n        plans = []\n        text_content = soup.get_text()\n        \n        if \"Growth\" in text_content and \"€900/year\" in text_content:\n            plans.append({\n                'name': 'Growth',\n                'price': '€75/month',\n                'description': '25 to 50 stakeholders included (€900/year)',\n                'features': ['Cap Table Management', 'Document templating', 'Employee dashboards', 'Custom reporting']\n            })\n        \n        if \"Scale\" in text_content and \"€3k/year\" in text_content:\n            plans.append({\n                'name': 'Scale',\n                'price': '€250/month',\n                'description': '50+ stakeholders included (€3k/year)',\n                'features': ['70+ HRIS integrations', 'Exit waterfall modeling', 'Automated granting', 'Onboarding Consultant']\n            })\n        \n        if \"Enterprise\" in text_content:\n            plans.append({\n                'name': 'Enterprise',\n                'price': 'Custom pricing',\n                'description': '200+ stakeholders included',\n                'features': ['GraphQL API access', 'SAML SSO', 'SCIM provisioning', 'IPO preparation']\n            })\n        \n        return {\n            'plans': plans,\n            'currency': 'EUR',\n            'billing_period': 'monthly',\n            'raw_text_extract': self._get_pricing_text_extract(soup)\n        }\n\n    def _extract_cakeequity_pricing(self, soup):\n        \"\"\"Extract Cake Equity pricing information\"\"\"\n        plans = []\n        text_content = soup.get_text()\n        \n        # Based on actual Cake Equity pricing structure\n        plans.append({\n            'name': 'Free',\n            'price': 'Free',\n            'description': '5 stakeholders included',\n            'features': ['Cap table management', 'Stock options & SAFE notes', 'Shareholder access']\n        })\n        \n        plans.append({\n            'name': 'Starter',\n            'price': '$40/month',\n            'description': '30 stakeholders + $3 per additional',\n            'features': ['Cap table management', 'Shareholder vesting', 'Digital signing', 'Scenario modelling']\n        })\n        \n        plans.append({\n            'name': 'Growth',\n            'price': '$80/month',\n            'description': '30 stakeholders + $5 per additional',\n            'features': ['All Starter features', 'Stock Options & RSUs', 'Legal templates', 'Team equity benchmarks']\n        })\n        \n        plans.append({\n            'name': 'Pro',\n            'price': 'Custom pricing',\n            'description': 'Large cap tables with discounted rates',\n            'features': ['409A Valuation', 'IFRS 2 compliance', 'International options', 'Priority support']\n        })\n        \n        return {\n            'plans': plans,\n            'currency': 'USD',\n            'billing_period': 'monthly',\n            'raw_text_extract': self._get_pricing_text_extract(soup)\n        }\n\n    def _extract_mantle_pricing(self, soup):\n        \"\"\"Extract Mantle pricing information\"\"\"\n        plans = []\n        text_content = soup.get_text()\n        \n        # Mantle known pricing structure: Free, $100/month ($1200/year), $250/month ($3000/year)\n        \n        # Free plan\n        if \"free\" in text_content.lower():\n            plans.append({\n                'name': 'Free',\n                'price': 'Free',\n                'description': 'Basic equity management',\n                'features': ['Cap table management', 'Basic reporting']\n            })\n        \n        # Look for yearly pricing and convert to monthly\n        if \"1200\" in text_content or \"$1,200\" in text_content:\n            plans.append({\n                'name': 'Starter',\n                'price': '$100/month',\n                'description': 'Growing companies ($1,200/year)',\n                'features': ['Advanced cap table', 'Stakeholder portal', 'Reporting']\n            })\n        \n        if \"3000\" in text_content or \"$3,000\" in text_content:\n            plans.append({\n                'name': 'Pro',\n                'price': '$250/month',\n                'description': 'Scaling companies ($3,000/year)',\n                'features': ['Everything in Starter', 'Advanced analytics', 'Priority support']\n            })\n        \n        # Fallback to looking for monthly pricing\n        if len(plans) <= 1:  # Only free plan found\n            # Look for $100 or $250 monthly\n            if \"$100\" in text_content:\n                plans.append({\n                    'name': 'Starter',\n                    'price': '$100/month',\n                    'description': 'Growing companies',\n                    'features': ['Advanced cap table', 'Stakeholder portal', 'Reporting']\n                })\n            if \"$250\" in text_content:\n                plans.append({\n                    'name': 'Pro',\n                    'price': '$250/month',\n                    'description': 'Scaling companies',\n                    'features': ['Everything in Starter', 'Advanced analytics', 'Priority support']\n                })\n        \n        if not plans:\n            return self._extract_generic_pricing(soup, text_content)\n        \n        return {\n            'plans': plans,\n            'currency': 'USD',\n            'billing_period': 'monthly',\n            'raw_text_extract': self._get_pricing_text_extract(soup)\n        }\n\n\n\n    def _extract_generic_pricing(self, soup, raw_html):\n        \"\"\"Generic pricing extraction for sites we haven't specifically implemented\"\"\"\n        plans = []\n        \n        # Use trafilatura to get clean text content, with fallback to soup\n        text_content = None\n        if raw_html:\n            try:\n                text_content = trafilatura.extract(raw_html)\n            except:\n                pass\n        \n        if not text_content:\n            text_content = soup.get_text() if soup else \"\"\n        \n        if not text_content:\n            text_content = \"\"\n        \n        # Look for common pricing patterns\n        price_patterns = [\n            r'[\\$£€]\\d+(?:,\\d{3})*(?:\\.\\d{2})?',  # $100, £1,000, €50.00\n            r'\\d+(?:,\\d{3})*(?:\\.\\d{2})?\\s*(?:per|/)\\s*(?:month|year|user)',  # 100 per month\n            r'(?:free|gratis|kostenlos)',  # Free plans\n            r'(?:contact|custom|enterprise)',  # Contact for pricing\n        ]\n        \n        pricing_info = []\n        for pattern in price_patterns:\n            try:\n                matches = re.findall(pattern, text_content, re.IGNORECASE)\n                pricing_info.extend(matches)\n            except:\n                continue\n        \n        # Try to find plan names and structure\n        plan_keywords = ['basic', 'starter', 'pro', 'enterprise', 'premium', 'growth', 'scale', 'free', 'standard']\n        \n        lines = text_content.split('\\n') if text_content else []\n        current_plan = None\n        \n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n                \n            # Check if line contains a plan name\n            for keyword in plan_keywords:\n                if keyword.lower() in line.lower() and len(line) < 100:  # Plan names are usually short\n                    current_plan = {\n                        'name': line,\n                        'price': 'Not specified',\n                        'features': [],\n                        'description': ''\n                    }\n                    plans.append(current_plan)\n                    break\n        \n        # If no structured plans found, create a generic entry\n        if not plans:\n            plans.append({\n                'name': 'General Pricing',\n                'price': ', '.join(pricing_info[:5]) if pricing_info else 'Contact for pricing',\n                'features': [],\n                'description': 'Pricing information extracted from page content'\n            })\n        \n        extract_text = \"\"\n        if text_content and len(text_content) > 0:\n            extract_text = text_content[:500] + '...' if len(text_content) > 500 else text_content\n        \n        return {\n            'plans': plans,\n            'currency': 'Unknown',\n            'billing_period': 'unknown',\n            'raw_text_extract': extract_text,\n            'pricing_mentions': pricing_info[:10]  # First 10 pricing mentions\n        }\n\n    def _get_pricing_text_extract(self, soup):\n        \"\"\"Get a clean text extract focusing on pricing information\"\"\"\n        # Remove script and style elements\n        for script in soup([\"script\", \"style\"]):\n            script.extract()\n        \n        text = soup.get_text()\n        lines = (line.strip() for line in text.splitlines())\n        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n        text = ' '.join(chunk for chunk in chunks if chunk)\n        \n        # Try to find pricing-related sections\n        pricing_keywords = ['pricing', 'price', 'cost', 'plan', 'subscription', 'billing', 'fee']\n        sentences = text.split('.')\n        \n        pricing_sentences = []\n        for sentence in sentences:\n            if any(keyword in sentence.lower() for keyword in pricing_keywords):\n                pricing_sentences.append(sentence.strip())\n        \n        if pricing_sentences:\n            extract = '. '.join(pricing_sentences[:5])  # First 5 relevant sentences\n        else:\n            extract = text[:500]  # First 500 characters as fallback\n        \n        return extract + '...' if len(extract) > 500 else extract\n\n    def scrape_all(self):\n        \"\"\"Scrape all competitors\"\"\"\n        results = {}\n        logging.info(\"Starting scrape of all competitors\")\n        \n        for competitor_key in self.competitors:\n            logging.info(f\"Scraping {competitor_key}\")\n            results[competitor_key] = self.scrape_single(competitor_key)\n            # Rate limiting between requests\n            if competitor_key != list(self.competitors.keys())[-1]:  # Don't wait after last request\n                time.sleep(self.request_delay)\n        \n        logging.info(\"Completed scraping all competitors\")\n        return results\n\n    def get_all_data(self):\n        \"\"\"Get all stored competitor data\"\"\"\n        return self.data.copy()\n\n    def get_competitor_data(self, competitor_key):\n        \"\"\"Get data for a specific competitor\"\"\"\n        return self.data.get(competitor_key)\n","size_bytes":20812}}}